{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010406ba",
   "metadata": {},
   "source": [
    "# Large Image Land Cover Classification\n",
    "\n",
    "This notebook demonstrates how to perform land cover classification on large GeoTIFF images using a sliding window approach.\n",
    "\n",
    "The process:\n",
    "1. Load a large GeoTIFF image\n",
    "2. Break it into overlapping 256x256 chips\n",
    "3. Run inference on each chip\n",
    "4. Stitch predictions back together\n",
    "5. Save as a georeferenced GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from box import Box\n",
    "import numpy as np\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from claymodel.finetune.segment.chesapeake_model import ChesapeakeSegmentor\n",
    "from large_image_inference import LargeImageSegmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b48b0",
   "metadata": {},
   "source": [
    "## 1. Define Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.cwd()  # Current directory (repo root)\n",
    "\n",
    "# Model checkpoints\n",
    "CHESAPEAKE_CHECKPOINT = ROOT_PATH / \"checkpoints\" / \"chesapeake-7class-segment_epoch-27_val-iou-0.8794.ckpt\"\n",
    "CLAY_CHECKPOINT = ROOT_PATH / \"checkpoints\" / \"clay-v1.5.ckpt\"\n",
    "METADATA_PATH = ROOT_PATH / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Input/output paths - UPDATE THESE WITH YOUR FILE PATHS\n",
    "INPUT_IMAGE_DIR = ROOT_PATH / \"data/cvpr/files/train\" \n",
    "INPUT_IMAGES = list(INPUT_IMAGE_DIR.glob(\"*naip*.tif\"))\n",
    "INPUT_IMAGE = random.choice(INPUT_IMAGES)\n",
    "\n",
    "OUTPUT_IMAGE = ROOT_PATH / \"output\" / \"prediction.tif\"  # Where to save the prediction\n",
    "\n",
    "\n",
    "# Inference parameters\n",
    "CHIP_SIZE = 256  # Size of each chip\n",
    "STRIDE = 128  # Stride for sliding window (128 = 50% overlap)\n",
    "BATCH_SIZE = 16  # Number of chips to process at once\n",
    "DEVICE = \"mps\"  # \"cpu\", \"mps\", or \"cuda\"\n",
    "\n",
    "# Parallel workers - MUST be 0 for GPU devices (MPS/CUDA)\n",
    "NUM_WORKERS = 0 if DEVICE in [\"mps\", \"cuda\"] else 4\n",
    "USE_PARALLEL = NUM_WORKERS > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b6192",
   "metadata": {},
   "source": [
    "## 2. Load Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained segmentation model\n",
    "print(\"Loading model...\")\n",
    "model = ChesapeakeSegmentor.load_from_checkpoint(\n",
    "    checkpoint_path=CHESAPEAKE_CHECKPOINT,\n",
    "    ckpt_path=CLAY_CHECKPOINT,\n",
    ")\n",
    "\n",
    "# Load metadata for normalization\n",
    "with open(METADATA_PATH) as f:\n",
    "    metadata = Box(yaml.safe_load(f))\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01402b",
   "metadata": {},
   "source": [
    "## 3. Create the Large Image Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0277a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the segmentor\n",
    "segmentor = LargeImageSegmentor(\n",
    "    model=model,\n",
    "    metadata=metadata,\n",
    "    platform=\"naip\",\n",
    "    chip_size=CHIP_SIZE,\n",
    "    stride=STRIDE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    use_parallel=USE_PARALLEL,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"Segmentor configured:\")\n",
    "print(f\"  - Chip size: {CHIP_SIZE}x{CHIP_SIZE}\")\n",
    "print(f\"  - Stride: {STRIDE} (overlap: {100 * (1 - STRIDE/CHIP_SIZE):.0f}%)\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Parallel workers: {NUM_WORKERS if USE_PARALLEL else 'Disabled'}\")\n",
    "print(f\"  - Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f307d",
   "metadata": {},
   "source": [
    "## 4. Run Inference on Large Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on the large image\n",
    "prediction = segmentor.predict_large_image(INPUT_IMAGE, OUTPUT_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "print(f\"Prediction shape: {prediction.shape}\")\n",
    "print(f\"Unique classes: {np.unique(prediction)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "for class_id in np.unique(prediction):\n",
    "    count = np.sum(prediction == class_id)\n",
    "    percentage = 100 * count / prediction.size\n",
    "    print(f\"  Class {class_id}: {count:,} pixels ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d9a2b",
   "metadata": {},
   "source": [
    "## 5. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap for visualization\n",
    "class_labels = [\n",
    "    \"0: Water\",\n",
    "    \"1: Tree Canopy\",\n",
    "    \"2: Low Vegetation\",\n",
    "    \"3: Barren Land\",\n",
    "    \"4: Impervious (Other)\",\n",
    "    \"5: Impervious (Road)\",\n",
    "    \"6: No Data\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    (0, 0, 255, 255),      # Deep Blue for water\n",
    "    (34, 139, 34, 255),    # Forest Green for tree canopy\n",
    "    (154, 205, 50, 255),   # Yellow Green for low vegetation\n",
    "    (210, 180, 140, 255),  # Tan for barren land\n",
    "    (169, 169, 169, 255),  # Dark Gray for impervious (other)\n",
    "    (105, 105, 105, 255),  # Dim Gray for impervious (road)\n",
    "    (255, 255, 255, 255),  # White for no data\n",
    "]\n",
    "\n",
    "# Normalize colors to 0-1 range\n",
    "colors_normalized = [(r/255, g/255, b/255, a/255) for r, g, b, a in colors]\n",
    "cmap = ListedColormap(colors_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original image for comparison\n",
    "original_image = rxr.open_rasterio(INPUT_IMAGE)\n",
    "\n",
    "# Display side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Original image (RGB)\n",
    "rgb_image = original_image[:3, :, :].values.transpose(1, 2, 0)\n",
    "rgb_image = np.clip(rgb_image / rgb_image.max(), 0, 1)  # Normalize for display\n",
    "axes[0].imshow(rgb_image)\n",
    "axes[0].set_title(\"Original Image\", fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Prediction\n",
    "im = axes[1].imshow(prediction, cmap=cmap, vmin=0, vmax=6)\n",
    "axes[1].set_title(\"Land Cover Prediction\", fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Add legend\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "               markerfacecolor=colors_normalized[i], markersize=10)\n",
    "    for i in range(len(colors_normalized))\n",
    "]\n",
    "fig.legend(handles, class_labels, loc='lower center', ncol=4, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb897d",
   "metadata": {},
   "source": [
    "## 6. Optional: Create a Zoomed-in View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a region to zoom into (adjust these coordinates)\n",
    "row_start, row_end = 0, 512\n",
    "col_start, col_end = 0, 512\n",
    "\n",
    "# Extract the region\n",
    "rgb_zoom = rgb_image[row_start:row_end, col_start:col_end, :]\n",
    "pred_zoom = prediction[row_start:row_end, col_start:col_end]\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(rgb_zoom)\n",
    "axes[0].set_title(\"Original (Zoomed)\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pred_zoom, cmap=cmap, vmin=0, vmax=6)\n",
    "axes[1].set_title(\"Prediction (Zoomed)\", fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb9db5",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Performance Tips:\n",
    "- **GPU**: Set `DEVICE = \"cuda\"` or `DEVICE = \"mps\"` if you have a GPU available - this will be much faster\n",
    "- **Parallel Processing**: Set `USE_PARALLEL = True` and adjust `NUM_WORKERS` (typically 4-8) for faster data loading\n",
    "  - For CPU: Parallel workers load and normalize chips while model runs inference\n",
    "  - For GPU: Parallel workers reduce CPU bottleneck in data preparation\n",
    "  - Disable (`USE_PARALLEL = False`) if you encounter memory issues\n",
    "- **Batch Size**: Increase if you have more memory available\n",
    "- **Stride**: Larger stride = faster processing but less accurate at boundaries\n",
    "- **Overlap**: The default 50% overlap (stride = chip_size/2) provides good quality\n",
    "\n",
    "### Memory Considerations:\n",
    "- Very large images may need to be processed in tiles\n",
    "- Monitor memory usage and adjust batch size accordingly\n",
    "- The output prediction is kept in memory - for very large images, consider processing in sections\n",
    "- Parallel workers use additional memory - reduce `NUM_WORKERS` if needed\n",
    "\n",
    "### Output Format:\n",
    "- The output is saved as a GeoTIFF with the same CRS and georeference as the input\n",
    "- Can be opened in QGIS, ArcGIS, or any GIS software\n",
    "- Pixel values correspond to class IDs (0-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
