{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010406ba",
   "metadata": {},
   "source": [
    "# Large Image Land Cover Classification\n",
    "\n",
    "This notebook demonstrates how to perform land cover classification on large GeoTIFF images using a sliding window approach.\n",
    "\n",
    "The process:\n",
    "1. Load a large GeoTIFF image\n",
    "2. Break it into overlapping 256x256 chips\n",
    "3. Run inference on each chip\n",
    "4. Stitch predictions back together\n",
    "5. Save as a georeferenced GeoTIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from box import Box\n",
    "from matplotlib.colors import ListedColormap\n",
    "import rioxarray as rxr\n",
    "\n",
    "from claymodel.finetune.segment.chesapeake_model import ChesapeakeSegmentor\n",
    "from large_image_inference import LargeImageSegmentor  # Now in repo root"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b48b0",
   "metadata": {},
   "source": [
    "## 1. Define Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path.cwd()  # Current directory (repo root)\n",
    "\n",
    "# Model checkpoints\n",
    "CHESAPEAKE_CHECKPOINT = ROOT_PATH / \"checkpoints\" / \"segment\" / \"chesapeake-7class-segment_epoch-39_val-iou-0.8765.ckpt\"\n",
    "CLAY_CHECKPOINT = ROOT_PATH / \"checkpoints\" / \"clay-v1.5.ckpt\"\n",
    "METADATA_PATH = ROOT_PATH / \"configs\" / \"metadata.yaml\"\n",
    "\n",
    "# Input/output paths - UPDATE THESE WITH YOUR FILE PATHS\n",
    "INPUT_IMAGE_DIR = ROOT_PATH / \"data/cvpr/files/train\" \n",
    "INPUT_IMAGES = list(INPUT_IMAGE_DIR.glob(\"*naip*.tif\"))\n",
    "INPUT_IMAGE = random.choice(INPUT_IMAGES)\n",
    "\n",
    "OUTPUT_IMAGE = ROOT_PATH / \"output\" / \"prediction.tif\"  # Where to save the prediction\n",
    "\n",
    "\n",
    "# Inference parameters\n",
    "CHIP_SIZE = 256  # Size of each chip\n",
    "STRIDE = 256  # Stride for sliding window (128 = 50% overlap)\n",
    "BATCH_SIZE = 16  # Number of chips to process at once\n",
    "DEVICE = \"mps\"  # \"cpu\", \"mps\", or \"cuda\"\n",
    "\n",
    "# Parallel workers - MUST be 0 for GPU devices (MPS/CUDA)\n",
    "NUM_WORKERS = 0 if DEVICE in [\"mps\", \"cuda\"] else 4\n",
    "USE_PARALLEL = NUM_WORKERS > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b6192",
   "metadata": {},
   "source": [
    "## 2. Load Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained segmentation model\n",
    "print(\"Loading model...\")\n",
    "model = ChesapeakeSegmentor.load_from_checkpoint(\n",
    "    checkpoint_path=CHESAPEAKE_CHECKPOINT,\n",
    "    ckpt_path=CLAY_CHECKPOINT,\n",
    ")\n",
    "\n",
    "# Load metadata for normalization\n",
    "with open(METADATA_PATH) as f:\n",
    "    metadata = Box(yaml.safe_load(f))\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01402b",
   "metadata": {},
   "source": [
    "## 3. Create the Large Image Segmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0277a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the segmentor\n",
    "segmentor = LargeImageSegmentor(\n",
    "    model=model,\n",
    "    metadata=metadata,\n",
    "    platform=\"naip\",\n",
    "    chip_size=CHIP_SIZE,\n",
    "    stride=STRIDE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    use_parallel=USE_PARALLEL,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(\"Segmentor configured:\")\n",
    "print(f\"  - Chip size: {CHIP_SIZE}x{CHIP_SIZE}\")\n",
    "print(f\"  - Stride: {STRIDE} (overlap: {100 * (1 - STRIDE/CHIP_SIZE):.0f}%)\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Parallel workers: {NUM_WORKERS if USE_PARALLEL else 'Disabled'}\")\n",
    "print(f\"  - Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81f307d",
   "metadata": {},
   "source": [
    "## 4. Run Inference on Large Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on the large image\n",
    "prediction = segmentor.predict_large_image(INPUT_IMAGE, OUTPUT_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014b79c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Test a single chip prediction\n",
    "print(\"Testing single chip prediction...\")\n",
    "image, geo_data = segmentor.load_image(INPUT_IMAGE)\n",
    "chips, positions = segmentor.extract_chips(image)\n",
    "\n",
    "test_chip = chips[0]\n",
    "print(f\"Chip shape: {test_chip.shape}\")\n",
    "print(f\"Chip min/max: {test_chip.min():.2f} / {test_chip.max():.2f}\")\n",
    "\n",
    "# Get probability output\n",
    "prob_output = segmentor.predict_chips([test_chip], return_probs=True)\n",
    "print(f\"\\nProbability output shape: {prob_output.shape}\")\n",
    "print(f\"Probability output min/max: {prob_output.min():.4f} / {prob_output.max():.4f}\")\n",
    "print(f\"Probabilities sum to 1? {np.allclose(prob_output.sum(axis=1), 1.0)}\")\n",
    "\n",
    "# Get class prediction\n",
    "class_pred = np.argmax(prob_output[0], axis=0)\n",
    "print(f\"\\nClass prediction shape: {class_pred.shape}\")\n",
    "print(f\"Unique classes in single chip: {np.unique(class_pred)}\")\n",
    "print(f\"Class distribution:\")\n",
    "for c in np.unique(class_pred):\n",
    "    count = (class_pred == c).sum()\n",
    "    pct = 100 * count / class_pred.size\n",
    "    print(f\"  Class {c}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4516430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Test the stitching process\n",
    "print(\"\\nTesting stitching with first 10 chips...\")\n",
    "test_chips = chips[:10]\n",
    "test_positions = positions[:10]\n",
    "\n",
    "# Get probabilities\n",
    "probs = segmentor.predict_chips(test_chips, return_probs=True)\n",
    "print(f\"Probabilities shape: {probs.shape}\")\n",
    "print(f\"Each chip has {probs.shape[1]} classes\")\n",
    "\n",
    "# Check a few chip predictions before stitching\n",
    "for i in range(min(3, len(probs))):\n",
    "    chip_pred = np.argmax(probs[i], axis=0)\n",
    "    unique = np.unique(chip_pred)\n",
    "    print(f\"Chip {i} classes: {unique}\")\n",
    "\n",
    "# Now test stitching\n",
    "stitched = segmentor.stitch_predictions(probs, test_positions, (image.shape[1], image.shape[2]))\n",
    "print(f\"\\nStitched result shape: {stitched.shape}\")\n",
    "print(f\"Stitched unique classes: {np.unique(stitched)}\")\n",
    "print(f\"Stitched class distribution:\")\n",
    "for c in np.unique(stitched):\n",
    "    count = (stitched == c).sum()\n",
    "    pct = 100 * count / stitched.size\n",
    "    print(f\"  Class {c}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the stitched test result\n",
    "# Define colormap\n",
    "colors_normalized = [(0, 0, 1, 1), (34/255, 139/255, 34/255, 1), \n",
    "                     (154/255, 205/255, 50/255, 1), (210/255, 180/255, 140/255, 1),\n",
    "                     (169/255, 169/255, 169/255, 1), (105/255, 105/255, 105/255, 1),\n",
    "                     (1, 1, 1, 1)]\n",
    "cmap = ListedColormap(colors_normalized)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Show a region of the original image\n",
    "test_region = image[:3, :512, :512].transpose(1, 2, 0)\n",
    "test_region_vis = np.clip(test_region / test_region.max(), 0, 1)\n",
    "axes[0].imshow(test_region_vis)\n",
    "axes[0].set_title(\"Original Image (First 512x512)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show the stitched prediction for that region\n",
    "axes[1].imshow(stitched[:512, :512], cmap=cmap, vmin=0, vmax=6)\n",
    "axes[1].set_title(\"Stitched Prediction (First 512x512)\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Stitching is working! Classes are preserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3130b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results\n",
    "print(f\"Prediction shape: {prediction.shape}\")\n",
    "print(f\"Unique classes: {np.unique(prediction)}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "for class_id in np.unique(prediction):\n",
    "    count = np.sum(prediction == class_id)\n",
    "    percentage = 100 * count / prediction.size\n",
    "    print(f\"  Class {class_id}: {count:,} pixels ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ce634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar chart of class distribution\n",
    "class_ids, counts = np.unique(prediction, return_counts=True)\n",
    "\n",
    "plt.bar(class_ids, counts)\n",
    "plt.xlabel(\"Class ID\")\n",
    "plt.ylabel(\"Pixel Count\")\n",
    "plt.title(\"Class Distribution in Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check individual chip predictions\n",
    "print(\"Debugging chip predictions...\")\n",
    "image, geo_data = segmentor.load_image(INPUT_IMAGE)\n",
    "chips, positions = segmentor.extract_chips(image)\n",
    "\n",
    "# Test a few chips\n",
    "test_chips = chips[:5]\n",
    "test_preds = segmentor.predict_chips(test_chips)\n",
    "\n",
    "print(f\"\\nTesting {len(test_chips)} individual chips:\")\n",
    "for i, pred in enumerate(test_preds):\n",
    "    unique_classes = np.unique(pred)\n",
    "    print(f\"  Chip {i}: Classes found: {unique_classes}, shape: {pred.shape}\")\n",
    "    \n",
    "print(f\"\\nFull prediction unique classes: {np.unique(prediction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d9a2b",
   "metadata": {},
   "source": [
    "## 5. Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap for visualization\n",
    "class_labels = [\n",
    "    \"0: Water\",\n",
    "    \"1: Tree Canopy\",\n",
    "    \"2: Low Vegetation\",\n",
    "    \"3: Barren Land\",\n",
    "    \"4: Impervious (Other)\",\n",
    "    \"5: Impervious (Road)\",\n",
    "    \"6: No Data\",\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    (0, 0, 255, 255),      # Deep Blue for water\n",
    "    (34, 139, 34, 255),    # Forest Green for tree canopy\n",
    "    (154, 205, 50, 255),   # Yellow Green for low vegetation\n",
    "    (210, 180, 140, 255),  # Tan for barren land\n",
    "    (169, 169, 169, 255),  # Dark Gray for impervious (other)\n",
    "    (105, 105, 105, 255),  # Dim Gray for impervious (road)\n",
    "    (255, 255, 255, 255),  # White for no data\n",
    "]\n",
    "\n",
    "# Normalize colors to 0-1 range\n",
    "colors_normalized = [(r/255, g/255, b/255, a/255) for r, g, b, a in colors]\n",
    "cmap = ListedColormap(colors_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original image for comparison\n",
    "original_image = rxr.open_rasterio(INPUT_IMAGE)\n",
    "\n",
    "# Display side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Original image (RGB)\n",
    "rgb_image = original_image[:3, :, :].values.transpose(1, 2, 0)\n",
    "rgb_image = np.clip(rgb_image / rgb_image.max(), 0, 1)  # Normalize for display\n",
    "axes[0].imshow(rgb_image)\n",
    "axes[0].set_title(\"Original Image\", fontsize=14)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Prediction\n",
    "im = axes[1].imshow(prediction, cmap=cmap, vmin=0, vmax=6)\n",
    "axes[1].set_title(\"Land Cover Prediction\", fontsize=14)\n",
    "axes[1].axis('off')\n",
    "\n",
    "# Add legend\n",
    "handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', \n",
    "               markerfacecolor=colors_normalized[i], markersize=10)\n",
    "    for i in range(len(colors_normalized))\n",
    "]\n",
    "fig.legend(handles, class_labels, loc='lower center', ncol=4, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb897d",
   "metadata": {},
   "source": [
    "## 6. Optional: Create a Zoomed-in View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3aea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a region to zoom into (adjust these coordinates)\n",
    "row_start, row_end = 0, 512\n",
    "col_start, col_end = 0, 512\n",
    "\n",
    "# Extract the region\n",
    "rgb_zoom = rgb_image[row_start:row_end, col_start:col_end, :]\n",
    "pred_zoom = prediction[row_start:row_end, col_start:col_end]\n",
    "\n",
    "# Display\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].imshow(rgb_zoom)\n",
    "axes[0].set_title(\"Original (Zoomed)\", fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pred_zoom, cmap=cmap, vmin=0, vmax=6)\n",
    "axes[1].set_title(\"Prediction (Zoomed)\", fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb9db5",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Performance Tips:\n",
    "- **GPU**: Set `DEVICE = \"cuda\"` or `DEVICE = \"mps\"` if you have a GPU available - this will be much faster\n",
    "- **Parallel Processing**: Set `USE_PARALLEL = True` and adjust `NUM_WORKERS` (typically 4-8) for faster data loading\n",
    "  - For CPU: Parallel workers load and normalize chips while model runs inference\n",
    "  - For GPU: Parallel workers reduce CPU bottleneck in data preparation\n",
    "  - Disable (`USE_PARALLEL = False`) if you encounter memory issues\n",
    "- **Batch Size**: Increase if you have more memory available\n",
    "- **Stride**: Larger stride = faster processing but less accurate at boundaries\n",
    "- **Overlap**: The default 50% overlap (stride = chip_size/2) provides good quality\n",
    "\n",
    "### Memory Considerations:\n",
    "- Very large images may need to be processed in tiles\n",
    "- Monitor memory usage and adjust batch size accordingly\n",
    "- The output prediction is kept in memory - for very large images, consider processing in sections\n",
    "- Parallel workers use additional memory - reduce `NUM_WORKERS` if needed\n",
    "\n",
    "### Output Format:\n",
    "- The output is saved as a GeoTIFF with the same CRS and georeference as the input\n",
    "- Can be opened in QGIS, ArcGIS, or any GIS software\n",
    "- Pixel values correspond to class IDs (0-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "claymodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
